import os
from typing import List, Dict, Any, Optional
import json
from datetime import datetime, timedelta
import httpx
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import io
import base64
import re
from openai import OpenAI
from utils.config import OPENAI_API_KEY, EVENTGO_API_BASE
from dotenv import load_dotenv
from utils.logger import logger

# Load environment variables from .env file
load_dotenv(override=True)

# Initialize OpenAI client
logger.info("Initializing OpenAI client...")
client = OpenAI(api_key=OPENAI_API_KEY)
logger.info("OpenAI client initialized successfully")

# Create HTTP client with SSL verification disabled for development
logger.info("Creating HTTP client...")
http_client = httpx.AsyncClient(verify=False)
logger.info("HTTP client created successfully")

SYSTEM_PROMPT = """ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„æ´»å‹•åˆ†æåŠ©æ‰‹ï¼Œå¯ä»¥å¹«åŠ©ç”¨æˆ¶æŸ¥è©¢å’Œåˆ†ææ´»å‹•è³‡è¨Šã€‚
ä½ å¯ä»¥ï¼š
1. æœå°‹ç‰¹å®šé¡å‹çš„æ´»å‹•
2. åˆ†ææ´»å‹•çš„æ™‚é–“åˆ†å¸ƒ
3. åˆ†ææ´»å‹•çš„åœ°ç†åˆ†å¸ƒ
4. æä¾›æ´»å‹•æ¨è–¦
5. ç”Ÿæˆæ´»å‹•åˆ†æå ±å‘Š

ç•¶ç”¨æˆ¶è©¢å•æ´»å‹•ç›¸é—œå•é¡Œæ™‚ï¼Œä½ éœ€è¦ï¼š
1. ç†è§£ç”¨æˆ¶çš„åå¥½å’Œéœ€æ±‚
2. æå–é—œéµè³‡è¨Šï¼ˆåœ°é»ã€ä¸»é¡Œã€æ—¥æœŸç­‰ï¼‰
3. ä½¿ç”¨çµæ§‹åŒ–æŸ¥è©¢ä¾†æœå°‹æ´»å‹•
4. æä¾›å€‹æ€§åŒ–çš„æ´»å‹•æ¨è–¦

è«‹æ ¹æ“šç”¨æˆ¶çš„éœ€æ±‚ï¼Œä½¿ç”¨é©ç•¶çš„APIä¾†ç²å–å’Œåˆ†ææ•¸æ“šã€‚"""

async def extract_search_params(message: str, existing_params: Dict[str, Any] = None) -> Dict[str, Any]:
    """
    Extract search parameters from natural language message using GPT-4
    existing_params: å·²å­˜åœ¨çš„æœå°‹åƒæ•¸ï¼ˆä¾‹å¦‚å¾å‰ç«¯å‚³ä¾†çš„æ’åºåƒæ•¸ï¼‰
    """
    logger.info(f"Extracting search parameters from message: {message}")
    logger.info(f"Existing parameters: {existing_params}")
    
    # Get today's date in different formats for the prompt
    today = datetime.now()
    today_str = today.strftime("%Y-%m-%d")
    today_timestamp = int(today.timestamp() * 1000)  # Convert to milliseconds
    
    # Define valid cities
    valid_cities = [
        "è‡ºåŒ—", "æ–°åŒ—", "è‡ºä¸­", "è‡ºå—", "é«˜é›„", "æ¡ƒåœ’", "åŸºéš†", "æ–°ç«¹", "å˜‰ç¾©",
        "è‹—æ —", "å½°åŒ–", "å—æŠ•", "é›²æ—", "å±æ±", "å®œè˜­", "èŠ±è“®", "è‡ºæ±", "æ¾æ¹–",
        "é‡‘é–€", "é€£æ±Ÿ"
    ]
    
    # Define valid sort keys
    valid_sort_keys = [
        "_score", "start_time", "end_time", "updated_time", "distance"
    ]
    
    prompt = f"""è«‹å¾ä»¥ä¸‹ç”¨æˆ¶è¨Šæ¯ä¸­æå–æ´»å‹•æœå°‹åƒæ•¸ï¼Œä¸¦ä»¥JSONæ ¼å¼è¿”å›ï¼š
    ç”¨æˆ¶è¨Šæ¯ï¼š{message}
    
    ä»Šå¤©çš„æ—¥æœŸæ˜¯ï¼š{today_str}ï¼ˆæ™‚é–“æˆ³ï¼š{today_timestamp}ï¼‰
    
    è«‹ç¢ºä¿è¿”å›çš„æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼ï¼ŒåŒ…å«ä»¥ä¸‹åƒæ•¸ï¼ˆå¦‚æœå­˜åœ¨ï¼‰ï¼š
    - query: æœå°‹é—œéµå­—
    - city: åŸå¸‚ï¼ˆå¿…é ˆæ˜¯ä»¥ä¸‹åŸå¸‚ä¹‹ä¸€æˆ–å¤šå€‹ï¼Œç”¨é€—è™Ÿåˆ†éš”ï¼š{', '.join(valid_cities)}ï¼‰
    - category: æ´»å‹•é¡åˆ¥
    - from: é–‹å§‹æ™‚é–“ï¼ˆæ¯«ç§’æ™‚é–“æˆ³ï¼Œä¾‹å¦‚ï¼š{today_timestamp}ï¼‰
    - to: çµæŸæ™‚é–“ï¼ˆæ¯«ç§’æ™‚é–“æˆ³ï¼‰
    - type: æ´»å‹•é¡å‹ï¼ˆé è¨­ç‚º "Web Post"ï¼‰
    - timeKey: æ™‚é–“éæ¿¾æ¢ä»¶ï¼ˆé è¨­ç‚º "start_time"ï¼‰
    - sort: æ’åºæ¬„ä½ï¼ˆé è¨­ç‚º "_score"ï¼Œå¯é¸å€¼ï¼š{', '.join(valid_sort_keys)}ï¼‰
    - asc: æ’åºæ–¹å‘ï¼ˆé è¨­ç‚º trueï¼Œè¡¨ç¤ºå‡åºï¼‰
    
    åŸå¸‚è™•ç†è¦å‰‡ï¼š
    1. å¦‚æœæåˆ°"å°ç£"æˆ–"å…¨å°"ï¼Œå‰‡åŒ…å«æ‰€æœ‰åŸå¸‚
    2. å¦‚æœæåˆ°å¤šå€‹åŸå¸‚ï¼Œç”¨é€—è™Ÿåˆ†éš”
    3. åŸå¸‚åç¨±å¿…é ˆå®Œå…¨åŒ¹é…ä»¥ä¸‹åˆ—è¡¨ï¼š{', '.join(valid_cities)}
    4. å¦‚æœæåˆ°çš„åŸå¸‚ä¸åœ¨åˆ—è¡¨ä¸­ï¼Œè«‹å¿½ç•¥è©²åŸå¸‚
    
    æ™‚é–“è™•ç†è¦å‰‡ï¼š
    1. å¦‚æœæ²’æœ‰æåˆ°ä»»ä½•æ™‚é–“ï¼Œé è¨­ä½¿ç”¨ä»Šå¤©çš„æ™‚é–“æˆ³ä½œç‚ºé–‹å§‹æ™‚é–“ï¼ˆfrom: {today_timestamp}ï¼‰
    2. å¦‚æœæåˆ°"ä»Šå¤©"ï¼Œä½¿ç”¨ä»Šå¤©çš„æ™‚é–“æˆ³
    3. å¦‚æœæåˆ°"æ˜å¤©"ï¼Œä½¿ç”¨ä»Šå¤©æ™‚é–“æˆ³ + 86400000ï¼ˆä¸€å¤©çš„æ¯«ç§’æ•¸ï¼‰
    4. å¦‚æœæåˆ°"ä¸‹é€±"ï¼Œä½¿ç”¨ä»Šå¤©æ™‚é–“æˆ³ + 604800000ï¼ˆä¸€é€±çš„æ¯«ç§’æ•¸ï¼‰
    5. å¦‚æœæåˆ°å…·é«”æ—¥æœŸï¼Œè«‹è½‰æ›ç‚ºå°æ‡‰çš„æ™‚é–“æˆ³
    6. å¦‚æœæåˆ°"éå»åŠå¹´"æˆ–"æœ€è¿‘åŠå¹´"ï¼Œè¨­ç½®ï¼š
       - from: ä»Šå¤©æ™‚é–“æˆ³ - 15552000000ï¼ˆåŠå¹´çš„æ¯«ç§’æ•¸ï¼‰
       - to: ä»Šå¤©æ™‚é–“æˆ³
    7. å¦‚æœæåˆ°"éå»ä¸€å€‹æœˆ"æˆ–"æœ€è¿‘ä¸€å€‹æœˆ"ï¼Œè¨­ç½®ï¼š
       - from: ä»Šå¤©æ™‚é–“æˆ³ - 2592000000ï¼ˆä¸€å€‹æœˆçš„æ¯«ç§’æ•¸ï¼‰
       - to: ä»Šå¤©æ™‚é–“æˆ³
    8. å¦‚æœæåˆ°"éå»ä¸€é€±"æˆ–"æœ€è¿‘ä¸€é€±"ï¼Œè¨­ç½®ï¼š
       - from: ä»Šå¤©æ™‚é–“æˆ³ - 604800000ï¼ˆä¸€é€±çš„æ¯«ç§’æ•¸ï¼‰
       - to: ä»Šå¤©æ™‚é–“æˆ³
    9. å¦‚æœæåˆ°"éå»ä¸€å¹´"æˆ–"æœ€è¿‘ä¸€å¹´"ï¼Œè¨­ç½®ï¼š
       - from: ä»Šå¤©æ™‚é–“æˆ³ - 31536000000ï¼ˆä¸€å¹´çš„æ¯«ç§’æ•¸ï¼‰
       - to: ä»Šå¤©æ™‚é–“æˆ³
    10. å¦‚æœæåˆ°"ä¸Šå€‹æœˆ"ï¼Œè¨­ç½®ï¼š
        - from: ä»Šå¤©æ™‚é–“æˆ³ - 5184000000ï¼ˆå…©å€‹æœˆçš„æ¯«ç§’æ•¸ï¼‰
        - to: ä»Šå¤©æ™‚é–“æˆ³ - 2592000000ï¼ˆä¸€å€‹æœˆçš„æ¯«ç§’æ•¸ï¼‰
    11. å¦‚æœæåˆ°"ä¸Šé€±"ï¼Œè¨­ç½®ï¼š
        - from: ä»Šå¤©æ™‚é–“æˆ³ - 1209600000ï¼ˆå…©é€±çš„æ¯«ç§’æ•¸ï¼‰
        - to: ä»Šå¤©æ™‚é–“æˆ³ - 604800000ï¼ˆä¸€é€±çš„æ¯«ç§’æ•¸ï¼‰
    
    æ’åºè¦å‰‡ï¼š
    1. å¦‚æœåŒ…å«é—œéµå­—æœç´¢ï¼ˆqueryï¼‰ï¼Œä½¿ç”¨ "_score" å’Œé™åºï¼ˆasc: falseï¼‰
    2. å¦‚æœæåˆ°"æœ€æ–°"ï¼Œä½¿ç”¨ "start_time" å’Œé™åºï¼ˆasc: falseï¼‰
    3. å¦‚æœæåˆ°"å³å°‡é–‹å§‹"ï¼Œä½¿ç”¨ "start_time" å’Œå‡åºï¼ˆasc: trueï¼‰
    4. å¦‚æœæåˆ°"å³å°‡çµæŸ"ï¼Œä½¿ç”¨ "end_time" å’Œå‡åºï¼ˆasc: trueï¼‰
    5. å¦‚æœæåˆ°"æœ€è¿‘æ›´æ–°"ï¼Œä½¿ç”¨ "updated_time" å’Œé™åºï¼ˆasc: falseï¼‰
    6. å¦‚æœæåˆ°"è·é›¢æœ€è¿‘"ï¼Œä½¿ç”¨ "distance" å’Œå‡åºï¼ˆasc: trueï¼‰
    7. å¦‚æœæ²’æœ‰æŒ‡å®šæ’åºï¼Œé è¨­ä½¿ç”¨ "start_time" å’Œå‡åºï¼ˆasc: trueï¼‰
    
    è«‹ç›´æ¥è¿”å›JSONæ ¼å¼ï¼Œä¸è¦æ·»åŠ ä»»ä½•markdownæ¨™è¨˜æˆ–å…¶ä»–æ–‡å­—ã€‚
    ä¾‹å¦‚ï¼Œå¦‚æœç”¨æˆ¶èªª"æ‰¾å°åŒ—çš„é‹å‹•æ´»å‹•"ï¼Œæ‡‰è©²è¿”å›ï¼š
    {{"city": "è‡ºåŒ—", "category": "é‹å‹•", "type": "Web Post", "timeKey": "start_time", "from": {today_timestamp}, "sort": "_score", "asc": false}}
    
    ä¾‹å¦‚ï¼Œå¦‚æœç”¨æˆ¶èªª"æ‰¾éå»åŠå¹´çš„å±•è¦½æ´»å‹•"ï¼Œæ‡‰è©²è¿”å›ï¼š
    {{"category": "å±•è¦½", "type": "Web Post", "timeKey": "start_time", "from": {today_timestamp - 15552000000}, "to": {today_timestamp}, "sort": "start_time", "asc": true}}
    
    ä¾‹å¦‚ï¼Œå¦‚æœç”¨æˆ¶èªª"æ‰¾ä¸Šå€‹æœˆçš„éŸ³æ¨‚æ´»å‹•"ï¼Œæ‡‰è©²è¿”å›ï¼š
    {{"category": "éŸ³æ¨‚", "type": "Web Post", "timeKey": "start_time", "from": {today_timestamp - 5184000000}, "to": {today_timestamp - 2592000000}, "sort": "start_time", "asc": true}}
    
    ä¾‹å¦‚ï¼Œå¦‚æœç”¨æˆ¶èªª"æ‰¾æœ€è¿‘ä¸€é€±çš„å±•è¦½"ï¼Œæ‡‰è©²è¿”å›ï¼š
    {{"category": "å±•è¦½", "type": "Web Post", "timeKey": "start_time", "from": {today_timestamp - 604800000}, "to": {today_timestamp}, "sort": "start_time", "asc": true}}
    
    è«‹æ³¨æ„ï¼š
    1. å¿…é ˆè¿”å›æœ‰æ•ˆçš„JSONæ ¼å¼
    2. å¦‚æœæŸå€‹åƒæ•¸ä¸å­˜åœ¨ï¼Œè«‹ä¸è¦åŒ…å«åœ¨JSONä¸­
    3. ä¸è¦æ·»åŠ ä»»ä½•é¡å¤–çš„æ–‡å­—èªªæ˜æˆ–markdownæ¨™è¨˜
    4. ä¸è¦ä½¿ç”¨```jsonæˆ–```ç­‰markdownæ¨™è¨˜
    5. type åƒæ•¸é è¨­å€¼ç‚º "Web Post"
    6. timeKey åƒæ•¸é è¨­å€¼ç‚º "start_time"
    7. city åƒæ•¸å¿…é ˆå®Œå…¨åŒ¹é…çµ¦å®šçš„åŸå¸‚åˆ—è¡¨
    8. å¦‚æœæ²’æœ‰æåˆ°æ™‚é–“ï¼Œå¿…é ˆåŒ…å« from åƒæ•¸ä¸¦è¨­ç½®ç‚ºä»Šå¤©çš„æ™‚é–“æˆ³
    9. å¦‚æœæœ‰é—œéµå­—æœç´¢ï¼Œsort åƒæ•¸æ‡‰è¨­ç‚º "_score"ï¼Œasc åƒæ•¸æ‡‰è¨­ç‚º false
    10. å¦‚æœæ²’æœ‰æŒ‡å®šæ’åºï¼Œsort åƒæ•¸æ‡‰è¨­ç‚º "start_time"ï¼Œasc åƒæ•¸æ‡‰è¨­ç‚º true
    11. æ™‚é–“ç¯„åœæŸ¥è©¢ï¼ˆå¦‚"éå»åŠå¹´"ï¼‰å¿…é ˆåŒæ™‚è¨­ç½® from å’Œ to åƒæ•¸
    """
    
    try:
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„æ´»å‹•æœå°‹åŠ©æ‰‹ï¼Œè«‹å¾ç”¨æˆ¶è¨Šæ¯ä¸­æå–æœå°‹åƒæ•¸ï¼Œä¸¦ä»¥JSONæ ¼å¼è¿”å›ã€‚è«‹ç¢ºä¿è¿”å›çš„æ˜¯ç´”JSONæ ¼å¼ï¼Œä¸è¦æ·»åŠ ä»»ä½•markdownæ¨™è¨˜æˆ–å…¶ä»–æ–‡å­—ã€‚type åƒæ•¸é è¨­å€¼ç‚º 'Web Post'ã€‚å¦‚æœæ²’æœ‰æåˆ°æ™‚é–“ï¼Œå¿…é ˆåŒ…å« from åƒæ•¸ä¸¦è¨­ç½®ç‚ºä»Šå¤©çš„æ™‚é–“æˆ³ã€‚"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3,
            max_tokens=500
        )
        
        content = response.choices[0].message.content.strip()
        logger.info(f"Raw response from GPT-4: {content}")
        
        # Clean the response content
        cleaned_content = content
        cleaned_content = re.sub(r'^```json\s*', '', cleaned_content)
        cleaned_content = re.sub(r'^```\s*', '', cleaned_content)
        cleaned_content = re.sub(r'\s*```$', '', cleaned_content)
        cleaned_content = cleaned_content.strip()
        
        logger.info(f"Cleaned content: {cleaned_content}")
        
        try:
            # Try to parse the cleaned content as JSON
            params = json.loads(cleaned_content)
            
            # Validate and process city parameter
            if 'city' in params:
                cities = params['city'].split(',')
                valid_cities_list = []
                for city in cities:
                    city = city.strip()
                    if city in valid_cities:
                        valid_cities_list.append(city)
                if valid_cities_list:
                    params['city'] = ','.join(valid_cities_list)
                else:
                    del params['city']
            
            # è™•ç†æ’åºåƒæ•¸çš„å„ªå…ˆç´š
            if existing_params:
                # å¦‚æœå‰ç«¯æŒ‡å®šäº†è¦ä¿ç•™æ’åºåƒæ•¸
                if existing_params.get('preserve_sort'):
                    params['sort'] = existing_params['sort']
                    logger.info(f"Using frontend sort parameter: {existing_params['sort']}")
                # å¦‚æœå‰ç«¯æŒ‡å®šäº†è¦ä¿ç•™æ’åºæ–¹å‘
                if existing_params.get('preserve_asc'):
                    params['asc'] = existing_params['asc']
                    logger.info(f"Using frontend asc parameter: {existing_params['asc']}")
            else:
                # æ ¹æ“šæœç´¢æ¢ä»¶è¨­ç½®é»˜èªæ’åº
                if 'query' in params:
                    # å¦‚æœæœ‰é—œéµå­—æœç´¢ï¼ŒæŒ‰ç›¸é—œåº¦æ’åº
                    params['sort'] = '_score'
                    params['asc'] = False
                elif 'sort' not in params:
                    # é»˜èªæŒ‰é–‹å§‹æ™‚é–“é™åº
                    params['sort'] = 'start_time'
                    params['asc'] = False
            
            # å¦‚æœå‰ç«¯æ²’æœ‰æŒ‡å®šè¦ä¿ç•™æ’åºåƒæ•¸ï¼Œä½†æä¾›äº†æ’åºåƒæ•¸ï¼Œå‰‡ä½¿ç”¨å‰ç«¯çš„æ’åºåƒæ•¸
            if existing_params and not existing_params.get('preserve_sort'):
                if 'sort' in existing_params:
                    params['sort'] = existing_params['sort']
                    logger.info(f"Using frontend sort parameter without preserve flag: {existing_params['sort']}")
                if 'asc' in existing_params:
                    params['asc'] = existing_params['asc']
                    logger.info(f"Using frontend asc parameter without preserve flag: {existing_params['asc']}")
            
            # Add default values if not present
            if 'type' not in params:
                params['type'] = 'Web Post'
            if 'from' not in params:
                params['from'] = today_timestamp
            
            logger.info(f"Successfully parsed JSON with priority handling: {params}")
            return params
        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse JSON: {str(e)}")
            # Try to extract JSON using regex as a fallback
            json_match = re.search(r'\{.*\}', cleaned_content)
            if json_match:
                try:
                    params = json.loads(json_match.group())
                    # è™•ç†æ’åºåƒæ•¸çš„å„ªå…ˆç´š
                    if existing_params:
                        # å¦‚æœå‰ç«¯æŒ‡å®šäº†è¦ä¿ç•™æ’åºåƒæ•¸
                        if existing_params.get('preserve_sort'):
                            params['sort'] = existing_params['sort']
                            logger.info(f"Using frontend sort parameter: {existing_params['sort']}")
                        # å¦‚æœå‰ç«¯æŒ‡å®šäº†è¦ä¿ç•™æ’åºæ–¹å‘
                        if existing_params.get('preserve_asc'):
                            params['asc'] = existing_params['asc']
                            logger.info(f"Using frontend asc parameter: {existing_params['asc']}")
                    else:
                        # æ ¹æ“šæœç´¢æ¢ä»¶è¨­ç½®é»˜èªæ’åº
                        if 'query' in params:
                            # å¦‚æœæœ‰é—œéµå­—æœç´¢ï¼ŒæŒ‰ç›¸é—œåº¦æ’åº
                            params['sort'] = '_score'
                            params['asc'] = False
                        elif 'sort' not in params:
                            # é»˜èªæŒ‰é–‹å§‹æ™‚é–“é™åº
                            params['sort'] = 'start_time'
                            params['asc'] = False
                    
                    # Add default values if not present
                    if 'type' not in params:
                        params['type'] = 'Web Post'
                    if 'from' not in params:
                        params['from'] = today_timestamp
                    
                    logger.info(f"Successfully extracted and parsed JSON with priority handling: {params}")
                    return params
                except json.JSONDecodeError as e:
                    logger.error(f"Failed to parse extracted JSON: {str(e)}")
                    return {
                        'type': 'Web Post',
                        'sort': 'start_time',
                        'asc': False,
                        'from': today_timestamp
                    }
            else:
                logger.error("No JSON found in response")
                return {
                    'type': 'Web Post',
                    'sort': 'start_time',
                    'asc': False,
                    'from': today_timestamp
                }
    except Exception as e:
        logger.error(f"Error extracting search parameters: {str(e)}", exc_info=True)
        return {
            'type': 'Web Post',
            'sort': 'start_time',
            'asc': False,
            'from': today_timestamp
        }

async def process_chat_message(message: str, chat_history: List[Dict[str, str]], page: int = 1, search_params: Dict[str, Any] = None) -> Dict[str, Any]:
    """
    Process chat message using OpenAI GPT-4 and handle event search
    """
    logger.info(f"Processing chat message: {message}")
    logger.info(f"Search params from frontend: {search_params}")
    
    # Handle None chat_history
    if chat_history is None:
        chat_history = []
    logger.info(f"Chat history length: {len(chat_history)}")
    logger.info(f"Current page: {page}")

    # Prepare messages for OpenAI
    messages = [
        {"role": "system", "content": SYSTEM_PROMPT},
        *chat_history,
        {"role": "user", "content": message}
    ]

    try:
        # Get response from OpenAI
        logger.info("Getting response from OpenAI...")
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=messages,
            temperature=0.7,
            max_tokens=1000
        )
        
        response_content = response.choices[0].message.content
        logger.info("Received response from OpenAI")
        
        # Check if the message is about event search
        if any(keyword in message.lower() for keyword in ["æ‰¾", "æœå°‹", "æ¨è–¦", "æ´»å‹•"]):
            logger.info("Message is about event search, extracting parameters...")
            # Extract search parameters with existing params
            logger.info(f"Frontend search parameters: {search_params}")
            extracted_params = await extract_search_params(message, search_params)
            logger.info(f"Extracted search parameters: {extracted_params}")
            
            if extracted_params:
                # Add p parameter to search params (EventGO API uses 'p' for pagination)
                extracted_params['p'] = page
                logger.info(f"Search parameters with page: {extracted_params}")
                
                logger.info("Searching for events with parameters...")
                # Search for events
                events = await recommend_events(extracted_params)
                
                # Format search parameters for display
                formatted_params = []
                if 'query' in extracted_params:
                    formatted_params.append(f"ğŸ” é—œéµå­—ï¼š{extracted_params['query']}")
                if 'city' in extracted_params:
                    formatted_params.append(f"ğŸ“ åŸå¸‚ï¼š{extracted_params['city']}")
                if 'category' in extracted_params:
                    formatted_params.append(f"ğŸ·ï¸ é¡åˆ¥ï¼š{extracted_params['category']}")
                if 'from' in extracted_params:
                    from_date = datetime.fromtimestamp(extracted_params['from'] / 1000)
                    formatted_params.append(f"ğŸ“… é–‹å§‹æ—¥æœŸï¼š{from_date.strftime('%Y-%m-%d')}")
                if 'to' in extracted_params:
                    to_date = datetime.fromtimestamp(extracted_params['to'] / 1000)
                    formatted_params.append(f"ğŸ“… çµæŸæ—¥æœŸï¼š{to_date.strftime('%Y-%m-%d')}")
                if 'type' in extracted_params:
                    formatted_params.append(f"ğŸ“‹ æ´»å‹•é¡å‹ï¼š{extracted_params['type']}")
                if 'sort' in extracted_params:
                    sort_display = {
                        '_score': 'ç›¸é—œåº¦',
                        'start_time': 'é–‹å§‹æ™‚é–“',
                        'end_time': 'çµæŸæ™‚é–“',
                        'updated_time': 'æ›´æ–°æ™‚é–“',
                        'distance': 'è·é›¢'
                    }.get(extracted_params['sort'], extracted_params['sort'])
                    formatted_params.append(f"â†•ï¸ æ’åºæ–¹å¼ï¼š{sort_display}")
                if 'asc' in extracted_params:
                    formatted_params.append(f"â†•ï¸ æ’åºæ–¹å‘ï¼š{'å‡åº' if extracted_params['asc'] else 'é™åº'}")
                if search_params and search_params.get('preserve_sort'):
                    formatted_params.append("â†•ï¸ æ’åºæ–¹å¼ï¼šä½¿ç”¨å‰ç«¯æŒ‡å®šçš„æ’åº")
                if search_params and search_params.get('preserve_asc'):
                    formatted_params.append("â†•ï¸ æ’åºæ–¹å‘ï¼šä½¿ç”¨å‰ç«¯æŒ‡å®šçš„æ–¹å‘")
                
                # Generate personalized response
                logger.info("Generating personalized response...")
                prompt = f"""æ ¹æ“šä»¥ä¸‹æœå°‹çµæœï¼Œç”Ÿæˆä¸€å€‹è‡ªç„¶çš„å›æ‡‰ï¼š
                æœå°‹åƒæ•¸ï¼š{json.dumps(extracted_params, ensure_ascii=False)}
                æœå°‹çµæœï¼š{json.dumps(events, ensure_ascii=False)}
                
                è«‹åŒ…å«ä»¥ä¸‹å…§å®¹ï¼š
                1. ç¬¦åˆæ¢ä»¶çš„æ´»å‹•æ•¸é‡ï¼ˆå…±æ‰¾åˆ° {events['pagination']['total_events']} å€‹æ´»å‹•ï¼Œæœ¬é é¡¯ç¤º {events['pagination']['current_page_count']} å€‹æ´»å‹•ï¼‰
                2. ä¸»è¦æ´»å‹•é¡å‹å’Œåœ°é»
                3. ç‰¹åˆ¥æ¨è–¦çš„æ´»å‹•ï¼Œæ¯å€‹æ´»å‹•è«‹åŒ…å«ï¼š
                   - æ´»å‹•åç¨±
                   - æ´»å‹•é–‹å§‹æ—¥æœŸå’ŒçµæŸæ—¥æœŸï¼ˆè«‹å°‡æ™‚é–“æˆ³è½‰æ›ç‚ºå¯è®€çš„æ—¥æœŸæ™‚é–“æ ¼å¼ï¼‰
                   - æ´»å‹•åœ°é»
                   - åŸå¸‚ï¼ˆå¦‚æœæœ‰ï¼‰
                   - å€åŸŸï¼ˆå¦‚æœæœ‰ï¼‰
                   - é¡åˆ¥ï¼ˆå¦‚æœæœ‰ï¼‰
                   - æ´»å‹•ä¾†æºé€£çµ
                
                æ³¨æ„äº‹é …ï¼š
                1. æ—¥æœŸæ™‚é–“æ ¼å¼è«‹ä½¿ç”¨ï¼šYYYY-MM-DD
                2. é€£çµè«‹ç¢ºä¿æ˜¯å®Œæ•´çš„URLï¼Œä¸è¦å› ç‚ºæ‹¬è™Ÿé€ æˆé€£çµéŒ¯èª¤
                3. å¦‚æœæ´»å‹•æ²’æœ‰çµæŸæ™‚é–“ï¼Œåªé¡¯ç¤ºé–‹å§‹æ™‚é–“
                4. æ´»å‹•åœ°é»é¡¯ç¤ºè¦å‰‡ï¼š
                   - å¦‚æœæ´»å‹•æœ‰GPSåº§æ¨™ï¼ˆlatitudeå’Œlongitudeï¼‰ï¼Œè«‹ä½¿ç”¨ä»¥ä¸‹æ ¼å¼ï¼š
                     [åœ°é»åç¨±](https://www.google.com/maps/place/{{latitude}},{{longitude}})
                     ä¾‹å¦‚ï¼šå¦‚æœæ´»å‹•çš„latitudeæ˜¯25.0150627ï¼Œlongitudeæ˜¯121.2188074ï¼Œå‰‡é€£çµæ‡‰è©²æ˜¯ï¼š
                     [å°åŒ—å¸‚ç«‹é«”è‚²é¤¨](https://www.google.com/maps/place/25.0150627,121.2188074)
                   - å¦‚æœæ´»å‹•æ²’æœ‰GPSåº§æ¨™ï¼Œç›´æ¥é¡¯ç¤ºåœ°é»åç¨±ï¼Œä¸è¦æ·»åŠ ä»»ä½•é€£çµ
                     ä¾‹å¦‚ï¼šå°åŒ—å¸‚ç«‹é«”è‚²é¤¨
                5. åŸå¸‚ã€å€åŸŸå’Œé¡åˆ¥è«‹åˆ†åˆ¥å–®ç¨é¡¯ç¤ºï¼Œæ ¼å¼ç‚ºï¼š
                   åŸå¸‚ï¼šå°åŒ—å¸‚
                   å€åŸŸï¼šä¿¡ç¾©å€
                   é¡åˆ¥ï¼šé‹å‹•
                6. å¦‚æœæŸé …ä¿¡æ¯ä¸å­˜åœ¨ï¼Œè«‹ä¸è¦é¡¯ç¤ºè©²é …
                7. åœ¨å›æ‡‰é–‹é ­å¿…é ˆé¡¯ç¤ºç¸½æ´»å‹•æ•¸å’Œç•¶å‰é é¡¯ç¤ºçš„æ´»å‹•æ•¸ï¼Œæ ¼å¼ç‚ºï¼š
                   "å…±æ‰¾åˆ° {events['pagination']['total_events']} å€‹æ´»å‹•ï¼Œæœ¬é é¡¯ç¤º {events['pagination']['current_page_count']} å€‹æ´»å‹•"
                
                è«‹ä»¥markdownæ ¼å¼è¿”å›ï¼Œç¢ºä¿é€£çµå¯ä»¥æ­£å¸¸é»æ“Šã€‚
                """
                
                recommendation_response = client.chat.completions.create(
                    model="gpt-4o",
                    messages=[
                        {"role": "system", "content": "ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„æ´»å‹•æ¨è–¦åŠ©æ‰‹ï¼Œè«‹æ ¹æ“šæœå°‹çµæœç”Ÿæˆè©³ç´°çš„æ´»å‹•æ¨è–¦å›æ‡‰ã€‚"},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0.7,
                    max_tokens=1000
                )
                
                # Add formatted search parameters to the response
                formatted_response = recommendation_response.choices[0].message.content
                if formatted_params:
                    formatted_response += "\n\n### æœå°‹æ¢ä»¶\n" + "\n".join(formatted_params)
                
                logger.info("Generated personalized response")
                logger.info(f"Pagination data in response: {json.dumps(events['pagination'], ensure_ascii=False, indent=2)}")
                
                response_data = {
                    "message": formatted_response,
                    "events": events,
                    "search_params": extracted_params,
                    "pagination": events['pagination']
                }
                
                return response_data
        
        return {
            "message": response_content,
            "events": None,
            "search_params": None,
            "pagination": None
        }
    except Exception as e:
        logger.error(f"Error processing chat message: {str(e)}", exc_info=True)
        raise

async def analyze_monthly_trends(category: str = "è—è¡“") -> Dict[str, Any]:
    """
    Analyze monthly trends for a specific category using the date-histogram API
    """
    logger.info(f"Analyzing monthly trends for category: {category}")
    end_date = datetime.now()
    start_date = end_date - timedelta(days=180)  # éå»6å€‹æœˆ
    
    try:
        logger.info("Fetching data from EventGO date-histogram API...")
        response = await http_client.get(
            f"{EVENTGO_API_BASE}/activity/date-histogram",
            params={
                "interval": "1M",  # æœˆåº¦é–“éš”
                "group": "start_time",  # æŒ‰æ´»å‹•é–‹å§‹æ™‚é–“åˆ†çµ„
                "timezone": "Asia/Taipei",  # å°åŒ—æ™‚å€
                "from": int(start_date.timestamp() * 1000),  # é–‹å§‹æ™‚é–“ï¼ˆæ¯«ç§’ï¼‰
                "to": int(end_date.timestamp() * 1000),  # çµæŸæ™‚é–“ï¼ˆæ¯«ç§’ï¼‰
                "category": category,  # æŒ‡å®šé¡åˆ¥
            }
        )
        data = response.json()
        logger.info(f"Raw API response: {data}")
        
        # Handle empty data case
        if not data or not isinstance(data, list) or len(data) == 0:
            logger.warning(f"No data available for category '{category}' in the specified time period")
            return {
                "message": f"åœ¨éå»åŠå¹´å…§ï¼ˆ{start_date.strftime('%Y-%m-%d')} è‡³ {end_date.strftime('%Y-%m-%d')}ï¼‰æ²’æœ‰æ‰¾åˆ°ä»»ä½•{category}ç›¸é—œçš„æ´»å‹•æ•¸æ“šã€‚",
                "data": [],
                "visualization": None,
                "trend_analysis": {
                    "total_events": 0,
                    "average_events": 0,
                    "max_month": None,
                    "min_month": None,
                    "monthly_data": [],
                    "time_period": {
                        "start": start_date.strftime('%Y-%m-%d'),
                        "end": end_date.strftime('%Y-%m-%d')
                    }
                }
            }
        
        # Process data and create visualization
        logger.info("Creating visualization...")
        
        # è½‰æ›APIå›æ‡‰æ ¼å¼ç‚ºDataFrame
        processed_data = []
        for item in data:
            # å°‡æ¯«ç§’æ™‚é–“æˆ³è½‰æ›ç‚ºæ—¥æœŸ
            timestamp_ms = item.get('key', 0)
            date_obj = datetime.fromtimestamp(timestamp_ms / 1000)
            month_str = date_obj.strftime('%Y-%m')
            
            processed_data.append({
                'timestamp': timestamp_ms,
                'key_as_string': item.get('key_as_string', ''),
                'month': month_str,
                'count': item.get('value', 0)
            })
        
        df = pd.DataFrame(processed_data)
        logger.info(f"Processed DataFrame shape: {df.shape}")
        logger.info(f"DataFrame data: {df}")
        
        # å‰µå»ºæœˆåº¦è¶¨å‹¢åœ–
        plt.figure(figsize=(12, 6))
        sns.barplot(data=df, x='month', y='count', palette='viridis')
        plt.xticks(rotation=45)
        plt.title(f'{category}æ´»å‹•æœˆåº¦åˆ†å¸ƒè¶¨å‹¢', fontsize=16, pad=20)
        plt.xlabel('æœˆä»½', fontsize=12)
        plt.ylabel('æ´»å‹•æ•¸é‡', fontsize=12)
        plt.grid(axis='y', alpha=0.3)
        
        # æ·»åŠ æ•¸å€¼æ¨™ç±¤
        for i, v in enumerate(df['count']):
            plt.text(i, v + max(df['count']) * 0.01, str(int(v)), 
                    ha='center', va='bottom', fontsize=10)
        
        # Convert plot to base64
        img = io.BytesIO()
        plt.savefig(img, format='png', bbox_inches='tight', dpi=150)
        img.seek(0)
        plot_url = base64.b64encode(img.getvalue()).decode()
        plt.close()
        logger.info("Visualization created successfully")
        
        # Generate trend analysis
        total_events = int(df['count'].sum())
        average_events = float(df['count'].mean()) if not df.empty else 0
        
        max_month_data = None
        min_month_data = None
        if not df.empty:
            max_idx = df['count'].idxmax()
            min_idx = df['count'].idxmin()
            max_month_data = {
                'month': df.loc[max_idx, 'month'],
                'count': int(df.loc[max_idx, 'count'])
            }
            min_month_data = {
                'month': df.loc[min_idx, 'month'],
                'count': int(df.loc[min_idx, 'count'])
            }
        
        trend_analysis = {
            "total_events": total_events,
            "average_events": round(average_events, 1),
            "max_month": max_month_data,
            "min_month": min_month_data,
            "monthly_data": df[['month', 'count']].to_dict('records'),
            "time_period": {
                "start": start_date.strftime('%Y-%m-%d'),
                "end": end_date.strftime('%Y-%m-%d')
            }
        }
        
        # ç”Ÿæˆè¶¨å‹¢åˆ†ææ–‡å­—
        trend_message = f"åœ¨éå»åŠå¹´å…§ï¼ˆ{start_date.strftime('%Y-%m-%d')} è‡³ {end_date.strftime('%Y-%m-%d')}ï¼‰å…±æ‰¾åˆ° {total_events} å€‹{category}ç›¸é—œæ´»å‹•ã€‚"
        
        if max_month_data and min_month_data:
            trend_message += f" å…¶ä¸­ {max_month_data['month']} æ´»å‹•æœ€å¤šï¼ˆ{max_month_data['count']}å€‹ï¼‰ï¼Œ{min_month_data['month']} æ´»å‹•æœ€å°‘ï¼ˆ{min_month_data['count']}å€‹ï¼‰ã€‚"
        
        if average_events > 0:
            trend_message += f" å¹³å‡æ¯æœˆæœ‰ {average_events:.1f} å€‹æ´»å‹•ã€‚"
        
        return {
            "message": trend_message,
            "data": data,  # åŸå§‹APIå›æ‡‰
            "visualization": plot_url,
            "trend_analysis": trend_analysis
        }
        
    except Exception as e:
        logger.error(f"Error analyzing monthly trends: {str(e)}", exc_info=True)
        return {
            "message": f"åˆ†æ{category}æ´»å‹•è¶¨å‹¢æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}",
            "data": [],
            "visualization": None,
            "trend_analysis": {
                "total_events": 0,
                "average_events": 0,
                "max_month": None,
                "min_month": None,
                "monthly_data": [],
                "time_period": {
                    "start": start_date.strftime('%Y-%m-%d'),
                    "end": end_date.strftime('%Y-%m-%d')
                }
            }
        }

async def analyze_geographic_distribution(category: str = "è—è¡“") -> Dict[str, Any]:
    """
    Analyze geographic distribution of events
    """
    logger.info(f"Analyzing geographic distribution for category: {category}")
    try:
        logger.info("Fetching data from EventGO API...")
        
        # æ§‹å»ºAPIåƒæ•¸
        api_params = {
            "group": "city"
        }
        
        # åªæœ‰ç•¶ category ä¸æ˜¯ "å…¨éƒ¨" æ™‚æ‰æ·»åŠ  category åƒæ•¸
        if category and category != "å…¨éƒ¨":
            api_params["category"] = category
        
        response = await http_client.get(
            f"{EVENTGO_API_BASE}/activity/histogram",
            params=api_params
        )
        data = response.json()
        logger.info("Data fetched successfully")
        
        # Create visualization
        logger.info("Creating visualization...")
        df = pd.DataFrame(data)
        category_text = "å…¨éƒ¨" if category == "å…¨éƒ¨" or not category else category
        plt.figure(figsize=(12, 6))
        sns.barplot(data=df, x='key', y='value')
        plt.xticks(rotation=45)
        plt.title(f'{category_text} Events Distribution by City')
        
        # Convert plot to base64
        img = io.BytesIO()
        plt.savefig(img, format='png', bbox_inches='tight')
        img.seek(0)
        plot_url = base64.b64encode(img.getvalue()).decode()
        logger.info("Visualization created successfully")
        
        return {
            "data": data,
            "visualization": plot_url
        }
    except Exception as e:
        logger.error(f"Error analyzing geographic distribution: {str(e)}", exc_info=True)
        raise

async def recommend_events(preferences: Dict[str, Any]) -> Dict[str, Any]:
    """
    Recommend events based on user preferences
    """
    logger.info(f"Recommending events with preferences: {preferences}")
    try:
        # ç¢ºä¿æœ‰å¿…è¦çš„åˆ†é åƒæ•¸
        if 'num' not in preferences:
            preferences['num'] = 5  # æ¯é é¡¯ç¤º5å€‹æ´»å‹•
        
        # ç¢ºä¿æœ‰é ç¢¼åƒæ•¸
        if 'p' not in preferences:
            preferences['p'] = 1
        
        logger.info(f"Final API parameters: {preferences}")
        
        logger.info("Fetching events from EventGO API...")
        response = await http_client.get(
            f"{EVENTGO_API_BASE}/activity",
            params=preferences
        )
        data = response.json()
        logger.info(f"API Response status: {response.status_code}")
        logger.info(f"API Response data keys: {list(data.keys()) if isinstance(data, dict) else 'Not a dict'}")
        
        # Get events and total count from API response
        events = data.get('events', [])
        total_events = data.get('count', 0)
        query_time = data.get('queryTime', 0)
        
        logger.info(f"Total events from API: {total_events}")
        logger.info(f"Number of events in response: {len(events)}")
        
        # ç²å–åˆ†é åƒæ•¸
        current_page = preferences.get('p', 1)
        events_per_page = preferences.get('num', 5)
        
        # è¨ˆç®—ç¸½é æ•¸
        total_pages = max(1, (total_events + events_per_page - 1) // events_per_page)
        current_page_count = len(events)
        
        # Filter out unnecessary fields from each event
        filtered_events = []
        for event in events:
            # Create a copy of the event without heavy fields
            filtered_event = {k: v for k, v in event.items() if k not in ['description', 'highlight_description', 'imgs']}
            filtered_events.append(filtered_event)
        
        # Create pagination data
        pagination_data = {
            'current_page': current_page,
            'events_per_page': events_per_page,
            'total_events': total_events,
            'total_pages': total_pages,
            'current_page_count': current_page_count
        }
        
        logger.info(f"Created pagination data: {pagination_data}")
        
        # Create response with filtered events and pagination
        result = {
            'count': total_events,
            'queryTime': query_time,
            'events': filtered_events,
            'pagination': pagination_data
        }
        
        logger.info(f"Returning {len(filtered_events)} events for page {current_page}")
        return result
    except Exception as e:
        logger.error(f"Error recommending events: {str(e)}", exc_info=True)
        raise

def generate_analysis_report(data: Dict[str, Any]) -> str:
    """
    Generate a natural language analysis report
    """
    logger.info("Generating analysis report...")
    prompt = f"""è«‹æ ¹æ“šä»¥ä¸‹æ•¸æ“šç”Ÿæˆä¸€ä»½æ´»å‹•åˆ†æå ±å‘Šï¼š
    {json.dumps(data, ensure_ascii=False, indent=2)}
    
    è«‹åŒ…å«ä»¥ä¸‹å…§å®¹ï¼š
    1. æ´»å‹•ç¸½é«”è¶¨å‹¢
    2. ç†±é–€åœ°å€åˆ†æ
    3. æ´»å‹•é¡å‹åˆ†å¸ƒ
    4. å»ºè­°å’Œæ´å¯Ÿ
    """
    
    try:
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„æ´»å‹•åˆ†æå¸«ï¼Œè«‹æ ¹æ“šæ•¸æ“šç”Ÿæˆè©³ç´°çš„åˆ†æå ±å‘Šã€‚"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7,
            max_tokens=1000
        )
        
        report = response.choices[0].message.content
        logger.info("Analysis report generated successfully")
        return report
    except Exception as e:
        logger.error(f"Error generating analysis report: {str(e)}", exc_info=True)
        raise 